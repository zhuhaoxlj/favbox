"""
Batch Embedding Script

æ‰¹é‡å¤„ç†ä¹¦ç­¾çš„å‘é‡åŒ–ï¼Œæ”¯æŒä¸­æ–­ç»§ç»­å’Œè¿›åº¦æŠ¥å‘Šã€‚
"""

import asyncio
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List
from sqlalchemy import select, and_
from sqlalchemy.ext.asyncio import AsyncSession
import time

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.database import get_db
from app.models.bookmark import Bookmark
from app.services.embedding_service import get_embedding_service
from app.services.classification_service import get_classification_service
from app.models.category import Category


class BatchEmbedder:
    """
    æ‰¹é‡å‘é‡åŒ–å¤„ç†å™¨
    """

    def __init__(
        self,
        batch_size: int = 100,
        overwrite: bool = False,
        also_classify: bool = True
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            overwrite: æ˜¯å¦è¦†ç›–å·²æœ‰å‘é‡
            also_classify: æ˜¯å¦åŒæ—¶è¿›è¡ŒAIåˆ†ç±»
        """
        self.batch_size = batch_size
        self.overwrite = overwrite
        self.also_classify = also_classify
        self.embedding_service = None
        self.classification_service = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total": 0,
            "processed": 0,
            "success": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": None,
            "end_time": None
        }

    async def process_all_bookmarks(
        self,
        db: AsyncSession,
        user_id: int
    ) -> Dict:
        """
        å¤„ç†ç”¨æˆ·æ‰€æœ‰ä¹¦ç­¾çš„å‘é‡åŒ–

        Args:
            db: æ•°æ®åº“ä¼šè¯
            user_id: ç”¨æˆ·ID

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        self.stats["start_time"] = datetime.now()

        print(f"ğŸš€ Starting batch embedding for user {user_id}")
        print(f"   Batch size: {self.batch_size}")
        print(f"   Overwrite: {self.overwrite}")
        print(f"   Also classify: {self.also_classify}")

        # 1. åˆå§‹åŒ–æœåŠ¡
        try:
            self.embedding_service = get_embedding_service()
            if self.also_classify:
                self.classification_service = get_classification_service()
            print("âœ… Services initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize services: {e}")
            return self.stats

        # 2. è·å–éœ€è¦å¤„ç†çš„ä¹¦ç­¾
        query = select(Bookmark).where(Bookmark.user_id == user_id)

        if not self.overwrite:
            # åªå¤„ç†æ²¡æœ‰å‘é‡çš„ä¹¦ç­¾
            query = query.where(Bookmark.ai_embedding.is_(None))

        result = await db.execute(query)
        bookmarks = result.scalars().all()

        self.stats["total"] = len(bookmarks)

        if self.stats["total"] == 0:
            print("âœ… No bookmarks to process")
            return self.stats

        print(f"ğŸ“Š Found {self.stats['total']} bookmarks to process")
        print()

        # 3. å¦‚æœéœ€è¦åˆ†ç±»ï¼Œè·å–å¯ç”¨åˆ†ç±»
        categories = []
        if self.also_classify:
            cat_result = await db.execute(
                select(Category).where(Category.user_id == user_id)
            )
            categories = cat_result.scalars().all()
            print(f"ğŸ“ Found {len(categories)} categories")

        # 4. åˆ†æ‰¹å¤„ç†
        for i in range(0, len(bookmarks), self.batch_size):
            batch = bookmarks[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(bookmarks) + self.batch_size - 1) // self.batch_size

            print(f"ğŸ“¦ Processing batch {batch_num}/{total_batches} ({len(batch)} bookmarks)")

            await self._process_batch(db, batch, categories)

            # æ¯æ‰¹æ¬¡åæäº¤
            await db.commit()

            # è¿›åº¦æŠ¥å‘Š
            progress = (self.stats["processed"] / self.stats["total"]) * 100
            print(f"   Progress: {progress:.1f}%")
            print(f"   Success: {self.stats['success']}, Failed: {self.stats['failed']}, Skipped: {self.stats['skipped']}")
            print()

        # 5. åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆå¦‚æœæ‰€æœ‰ä¹¦ç­¾éƒ½å·²å‘é‡åŒ–ï¼‰
        await self._create_vector_indexes(db)

        self.stats["end_time"] = datetime.now()
        duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()

        print("=" * 60)
        print("âœ… Batch embedding completed!")
        print(f"   Total: {self.stats['total']}")
        print(f"   Processed: {self.stats['processed']}")
        print(f"   Success: {self.stats['success']}")
        print(f"   Failed: {self.stats['failed']}")
        print(f"   Skipped: {self.stats['skipped']}")
        print(f"   Duration: {duration:.1f}s ({duration/60:.1f} minutes)")
        print(f"   Average: {duration/self.stats['processed']:.2f}s per bookmark")
        print("=" * 60)

        return self.stats

    async def _process_batch(
        self,
        db: AsyncSession,
        bookmarks: List[Bookmark],
        categories: List[Category]
    ):
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡
        """
        # å‡†å¤‡æ•°æ®
        texts = [(bm.title, bm.description or "") for bm in bookmarks]

        try:
            # 1. æ‰¹é‡ç”Ÿæˆå‘é‡
            print(f"   ğŸ”„ Generating embeddings...")
            embeddings = await self.embedding_service.batch_generate_embeddings(texts)

            # 2. åŒæ—¶è¿›è¡Œåˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            classifications = []
            if self.also_classify and categories:
                print(f"   ğŸ¤– Classifying bookmarks...")
                for idx, bookmark in enumerate(bookmarks):
                    try:
                        cat_id, confidence, cat_name = await self.classification_service.classify_bookmark(
                            title=bookmark.title,
                            description=bookmark.description,
                            url=bookmark.url,
                            available_categories=categories
                        )
                        classifications.append({
                            "bookmark_id": bookmark.id,
                            "category_id": cat_id,
                            "confidence": confidence
                        })
                    except Exception as e:
                        print(f"      âš ï¸  Classification failed for {bookmark.id}: {e}")
                        classifications.append(None)

            # 3. æ›´æ–°ä¹¦ç­¾
            print(f"   ğŸ’¾ Updating bookmarks...")
            for idx, bookmark in enumerate(bookmarks):
                try:
                    # æ›´æ–°å‘é‡
                    bookmark.ai_embedding = embeddings[idx]
                    bookmark.last_ai_analysis_at = datetime.now()

                    # æ›´æ–°åˆ†ç±»
                    if self.also_classify and idx < len(classifications):
                        classification = classifications[idx]
                        if classification:
                            bookmark.ai_category_id = classification["category_id"]

                    self.stats["success"] += 1

                except Exception as e:
                    print(f"      âŒ Failed to update bookmark {bookmark.id}: {e}")
                    self.stats["failed"] += 1

                self.stats["processed"] += 1

        except Exception as e:
            print(f"   âŒ Batch processing failed: {e}")
            # æ•´ä¸ªæ‰¹æ¬¡æ ‡è®°ä¸ºå¤±è´¥
            self.stats["failed"] += len(bookmarks)
            self.stats["processed"] += len(bookmarks)

    async def _create_vector_indexes(self, db: AsyncSession):
        """åˆ›å»ºå‘é‡ç´¢å¼•"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•
            from sqlalchemy import text
            result = await db.execute(text("""
                SELECT indexname FROM pg_indexes
                WHERE tablename = 'bookmarks'
                  AND indexname LIKE '%embedding%'
            """))
            existing_indexes = result.fetchall()

            if existing_indexes:
                print(f"   âœ… Vector indexes already exist: {[r[0] for r in existing_indexes]}")
                return

            print("   ğŸ“Š Creating vector indexes...")

            # HNSWç´¢å¼• - ä½™å¼¦ç›¸ä¼¼åº¦
            await db.execute(text("""
                CREATE INDEX IF NOT EXISTS idx_bookmarks_embedding_hnsw
                ON bookmarks USING hnsw (ai_embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64)
            """))

            print("   âœ… Vector indexes created")

        except Exception as e:
            print(f"   âš ï¸  Failed to create indexes: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Batch embed bookmarks")
    parser.add_argument("--user-id", type=int, required=True, help="User ID")
    parser.add_argument("--batch-size", type=int, default=100, help="Batch size")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing embeddings")
    parser.add_argument("--no-classify", action="store_true", help="Skip classification")

    args = parser.parse_args()

    # è·å–æ•°æ®åº“ä¼šè¯
    async for db in get_db():
        embedder = BatchEmbedder(
            batch_size=args.batch_size,
            overwrite=args.overwrite,
            also_classify=not args.no_classify
        )

        await embedder.process_all_bookmarks(db, args.user_id)
        break


if __name__ == "__main__":
    asyncio.run(main())
